"""Forwarders Goals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RN0Y7DrNdvwXZZuEWMxzEi-Cz6_q6LpG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Read Data from csv
dfBest = pd.read_csv('Forwarders.csv')
dfBest_y = dfBest['Total Goals']
dfBestSize = len(dfBest)
dfBest.head()

"""Gradient

Gradient Descent
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy  
import matplotlib.pyplot as plt
errorsArray= [];  
yAux= []; 
alfa =.01  # learning rate
epochs = 0

"""Evaluates a generic linear function h(x) with current parameters. 
Args:
	params (lst) a list containing of element x params
	sample (lst) a list containing of element x values
Use:
	Null Hyphotesis: y = 0 (Beta) + (Beta1 * x1) +(Beta2 * x2) +(Beta3 * x3) ... 
Returns:
	Evaluation of h(x)
"""
def hypothesisEvaluation(params, sample):
	acum = 0
	for i in range(len(params)):
		acum = acum + (params[i]*sample[i])  #evaluates h(x) = a+bx1+cx2+ ... nxn.. 
	return acum;

"""Appends the errors generated by the estimated values of hyp and the real value y

Args:
	params (lst) a list containing the corresponding parameter for each element x of the sample
	samples (lst) a 2 dimensional list containing the input samples 
	y (lst) a list containing the corresponding real result for each sample
	show boolean value to show results

Use:
	Loss functions MSE
	MSE = 1/n SUM (Value - Original) ^2 

"""
def MSE(params, samples,y,show):
	error_acum =0
	for i in range(len(samples)):
		hypothesis = hypothesisEvaluation(params,samples[i])
		error_acum=+((hypothesis-y[i])**2)
		if(show == True):
			print( "Hypothesis: %f | y: %f " % (hypothesis,  y[i]))      
			yAux.append(hypothesis)   
	mse=error_acum/len(samples)
	errorsArray.append(mse)
 

"""Gradient Descent algorithm 
Args:
	params (lst) a list containing the corresponding parameter for each element x of the sample
	samples (lst) a 2 dimensional list containing the input samples 
	y (lst) a list containing the corresponding real result for each sample
	alfa(float) the learning rate
Returns:
	temp(lst) a list with the new values for the parameters after 1 run of the sample set
"""
def GD(params, samples, y, alfa):
	temp = list(params)
	general_error=0
	for j in range(len(params)):
		acum =0; error_acum=0
		for i in range(len(samples)):
			error = hypothesisEvaluation(params,samples[i]) - y[i]
			acum = acum + error*samples[i][j]  #Sumatory part of the Gradient Descent formula for linear Regression.
		temp[j] = params[j] - alfa*(1/len(samples))*acum  #Subtraction of original parameter value with learning rate included.
	return temp


"""Use Benji Method (add 1, to the list)
Args:
	params (lst) a list containing the corresponding parameter for each element x of the sample
Returns:
	samples(lst) a list with the bias added
"""
def getBias(samples):
  for i in range(len(samples)):
	  if isinstance(samples[i], list):
		  samples[i]=  [1]+samples[i]
	  else:
		  samples[i]=  [1,samples[i]]
  return samples


"""Normalizes sample values
Args:
	params (lst) a list containing the corresponding parameter for each element x of the sample
Returns:
	samples(lst) a list with the normalized version of the original samples
Use:
X normalized = x - Xmin/Xmax-Xmin
"""
def scaling(samples):

	acum =0
	samples = numpy.asarray(samples).T.tolist()  
	for i in range(1,len(samples)):	
		max_val = max(samples[i])
		min_val = min(samples[i])
		for j in range(len(samples[i])):
			samples[i][j] = (samples[i][j] - min_val)/(max_val - min_val)  #Min - Max scaling
	return numpy.asarray(samples).T.tolist()
 
"""Normalizes sample values
Args:
	params (lst) a list containing the corresponding parameter for each element x of the sample
	dfBest a Dataframe with the Data

Use:
Filter by position and get main Attributes
"""
def processPlayersData(samples, dfBest):
	# Getting attributes
	for i in range(dfBestSize):
		if (dfBest['Position'][i] == 'Forward'):
			samples.insert(i,[dfBest['Age'][i],dfBest['Appearances'][i],dfBest['Shots On Target'][i]])


def R2Calc(y_test, y_pred):
    # Calculate the mean of y
    y_mean = sum(y_test)/len(y_test)
    # Sum of Square Residuals
    SST = 0
    for i in range(len(y_test)):
        SST += (y_test[i] - y_mean) ** 2
    # Sum of Square Totals
    SSR = 0
    for i in range(len(y_test)):
        SSR += (y_test[i] - y_pred[i]) ** 2
    R2 = 1 - (SSR / SST)
    return R2
				
params = [0,0,0,0]
samples = []
processPlayersData(samples,dfBest)
X = samples
y = dfBest_y
getBias(samples)
print ("original samples:")
print (samples)
samples = scaling(samples)
print ("scaled samples:")
print (samples)

while True:  
	previousParams = list(params)
	MSE(params, samples, y, False)
	params=GD(params, samples,y,alfa)	
	epochs = epochs + 1
	if(previousParams == params or epochs == 2000):   #  local minima is found when there is no further improvement
		MSE(params, samples, y, True)
		print ("GD Params:")
		print (params)
		break
		

print('Coefficient of determination: %.2f'
#       % R2Calc(y, yAux))


#use this to generate a graph of the errors/loss so we can see whats going on (diagnostics)
plt.plot(errorsArray)
plt.show()

